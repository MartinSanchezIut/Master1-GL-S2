{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Ingénierie des données textuelles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDSvxMcyXMhM"
      },
      "source": [
        "<H1> Ingénierie des données textuelles </H1>  \n",
        "\n",
        "De nombreuses applications utilisent des données textuelles pour faire de la prédiction : détection d'opinions, classification automatique de documents en fonction du contenu : spam - no spam, article sport vs article économie, etc...   \n",
        "\n",
        "La classification se fait de manière tout à fait classique par contre il est indispensable de traiter les documents pour pouvoir les faire interpréter par un classifieur.\n",
        "Le traitement des données textuelles est particulièrement difficile car il dépend des données disponibles et tout traitement n'est pas forcément justifié. Par exemple le fait de convertir tout le texte en minuscule peut faire perdre de l'information (*e.g., Mr Play indique une personne et play un verbe*), la suppression des ponctuations peut avoir des conséquences (*! est très souvent utilisé pour la détection d'opinions*), etc. En outre chaque langue possède aussi ses particularités et les librairies disponibles considèrent souvent l'anglais même s'il existe de plus en plus de ressources en différentes langues comme le français.  \n",
        "\n",
        "Le but de ce notebook est de présenter différentes approches d'ingénierie de données textuelles afin de pré-traiter les données.   \n",
        "\n",
        "Comme nous le verrons tout au cours de ce notebook, il existe de nombreuses librairies qui offrent des fonctionnalités pour pouvoir facilement traiter les données. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQSC8frX5Ao"
      },
      "source": [
        "## Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zICeCUqYCfS"
      },
      "source": [
        "\n",
        "Avant de commencer, il est nécessaire de déjà posséder dans son environnement toutes les librairies utiles. Dans la seconde cellule nous importons toutes les librairies qui seront utiles à ce notebook. Il se peut que, lorsque vous lanciez l'éxecution de cette cellule, une soit absente. Dans ce cas il est nécessaire de l'installer. Pour cela dans la cellule suivante utiliser la commande :  \n",
        "\n",
        "*! pip install nom_librairie*  \n",
        "\n",
        "**Attention :** il est fortement conseillé lorsque l'une des librairies doit être installer de relancer le kernel de votre notebook.\n",
        "\n",
        "**Remarque :** même si toutes les librairies sont importées dès le début, les librairies utiles pour des fonctions présentées au cours de ce notebook sont ré-importées de manière à indiquer d'où elles viennent et ainsi faciliter la réutilisation de la fonction dans un autre projet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0OpnDBHYHmv"
      },
      "source": [
        "# utiliser cette cellule pour installer les librairies manquantes\n",
        "# pour cela il suffit de taper dans cette cellule : !pip install nom_librairie_manquante\n",
        "# d'exécuter la cellule et de relancer la cellule suivante pour voir si tout se passe bien\n",
        "# recommencer tant que toutes les librairies ne sont pas installées ...\n",
        "\n",
        "# sous Colab il faut déjà intégrer ces deux librairies\n",
        "\n",
        "!pip install langdetect \n",
        "!pip install contractions\n",
        "# eventuellement ne pas oublier de relancer le kernel du notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZw2C6r8YJWW"
      },
      "source": [
        "# Importation des différentes librairies utiles pour le notebook\n",
        "\n",
        "#Sickit learn met régulièrement à jour des versions et \n",
        "#indique des futurs warnings. \n",
        "#ces deux lignes permettent de ne pas les afficher.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# librairies générales\n",
        "import pickle \n",
        "import pandas as pd\n",
        "from scipy.stats import randint\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import base64\n",
        "import re\n",
        "import sys\n",
        "\n",
        "import contractions\n",
        "\n",
        "# librairie BeautifulSoup\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# librairie affichage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wordcloud\n",
        "\n",
        "## detection de language\n",
        "import langdetect \n",
        "\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk import RegexpParser\n",
        "# il est possible de charger l'ensemble des librairies en une seule fois \n",
        "# décocher le commentaire de la ligne ci-dessous\n",
        "#nltk.download('all') \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "nltk.download('tagsets')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "# il faut sélectionner pour quelle langue les traitements vont être faits.\n",
        "nlp = spacy.load('en')\n",
        "from spacy.lang.fr import French"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s2sF7a-am4I"
      },
      "source": [
        "Pour pouvoir sauvegarder sur votre répertoire Google Drive, il est nécessaire de fournir une autorisation. Pour cela il suffit d'éxecuter la ligne suivante et de saisir le code donné par Google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1zULJ9daqP6"
      },
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qe4Xtohau6v"
      },
      "source": [
        "Corriger éventuellement la ligne ci-dessous pour mettre le chemin vers un répertoire spécifique dans votre répertoire Google Drive : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahDl5RV9tVfp"
      },
      "source": [
        "my_local_drive='/content/gdrive/My Drive/Colab Notebooks/ML_FDS'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAYb7LUyDp0d"
      },
      "source": [
        "# fonctions utilities (affichage, confusion, etc.)\n",
        "from MyNLPUtilities import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPPQEQ_tbsjZ"
      },
      "source": [
        "## **Une première analyse des documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyRcDb0oby1i"
      },
      "source": [
        "Très souvent pour commencer à appréhender un texte, l'une des approches consiste à déjà regarder les mots principaux d'un texte. Les word clouds offrent cette fonctionnalité. Il est également utile de connaître la langue du document. Par exemple, cela va permettre de pouvoir utiliser des librairies spécifiques, supprimer des mots inutiles pour cette langue, etc.  \n",
        "\n",
        "Il existe heureusement des librairies spécifiques comme *wordcloud* ou *langdetect*.  \n",
        "\n",
        "Nous présentons par la suite quelques premiers petits traitements pratiques qui peuvent être effectués pour nettoyer un peu les données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZptVsWEibpT9"
      },
      "source": [
        "import wordcloud\n",
        "\n",
        "## detection de language\n",
        "import langdetect \n",
        "document = \"Alice opened the door and found that it led into a small passage, not much larger than a rat-hole: she knelt down and looked along the passage into the loveliest garden you ever saw. \\\n",
        "How she longed to get out of that dark hall, and wander about among those beds of bright flowers and those cool fountains, but she could not even get her head through the doorway\"\n",
        "\n",
        "# affichage des word clouds\n",
        "wc = wordcloud.WordCloud(background_color='black', max_words=100, \n",
        "                         max_font_size=35)\n",
        "wc = wc.generate(str(document))\n",
        "fig = plt.figure(num=1)\n",
        "plt.axis('off')\n",
        "plt.imshow(wc, cmap=None)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\" Le document '\", document, \"' est en  \", langdetect.detect(document))\n",
        "print (\"la phrase il fait beau est en \", langdetect.detect(\"il fait beau\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqybSzore6Y0"
      },
      "source": [
        "**Encodage des données**  \n",
        "Les données textuelles sont souvent sujettes à des problèmes d'encodage ( “Latin”, “UTF8” etc). Le plus simple est de les convertir dans un format classique (UTF8)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQj9Vyt-fGEN"
      },
      "source": [
        "import unicodedata\n",
        "chaine = u\"Klüft skräms inför på fédéral électoral große\"\n",
        "chaine=unicodedata.normalize('NFKD', chaine).encode('ascii','ignore')\n",
        "print (chaine)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSO8y_SdfRTc"
      },
      "source": [
        "**Suppression des tags XML/HTML**  \n",
        "Les données textuelles peuvent être issues de pages web, contenir des entêtes, etc..\n",
        "L'une des premières étapes consistent à les nettoyer pour ne retenir que le texte.\n",
        "La librairie BeautifulSoup permet de récupérer directement le texte en supprimant les tags : https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eljt27PWflvS"
      },
      "source": [
        "page = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html> <head> <title>Machine Learning - Apprentissage</title> </head>\n",
        "  <body>\n",
        " \n",
        "<h1>Le cours de Machine Learning est à a FDS </h1> (<a href=https://sciences.edu.umontpellier.fr>).\n",
        "\n",
        " Situé à Montpellier [où il fait toujours beau]\n",
        "</body> </html>\"\"\"\n",
        "print (page)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWqJAZIggFhp"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "page=strip_html (page)\n",
        "print (page)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzBMlEUYgk9z"
      },
      "source": [
        "**Utilisation d'expressions régulières**  \n",
        "\n",
        "De nombreuses modifications peuvent être réalisées en utilisant des expressions régulières (utilisation de la librairie *re*). Par exemple la fonction suivante permet de supprimer les textes entre crochets [].  \n",
        "\n",
        "Nous verrons d'autres exemples d'expressions régulières par la suite. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKYwtZf7g6lc"
      },
      "source": [
        "import re\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "page=remove_between_square_brackets(page)\n",
        "print (page)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9IeV4rqp3XT"
      },
      "source": [
        "## **Plus loin dans les pré-traitements des documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97PdhyHYqYwG"
      },
      "source": [
        "La phase de pré-traitement est la phrase de préparation des données pour que ces dernières soient utilisables par un modèle d'apprentissage. Outre l'étape de nettoyage des données,  il y a de nombreux pré-traitements qui peuvent ou doivent être effectués en fonction de leur type et de la tâche visée. Par exemple, l'extraction des tokens composant les phrases, la racinisation/\"stemmatisation\" qui vise à garder la racine des mots, la lemmatisation qui consiste à appliquer une analyse lexicale d'un texte, la suppression de mots vides ou creux (i.e., des mots qui ne sont pas discriminants pour la classification), la détection d'entité nommée, etc.   \n",
        "\n",
        " \n",
        "\n",
        "L'étape de nettoyage peut contenir elle même différentes sous-étapes selon les données (Cf. exemple précédent avec des données HTML) et la tâche visée. Elle comprend souvent la conversion des documents en minuscule et la suppression des signes de ponctuations.   \n",
        "\n",
        "Comme nous l'avons vu précédemment, il existe de nombreuses librairies pour effectuer ces différentes tâches. Dans ce notebook nous nous intéresserons plus particulièrement à \n",
        " :\n",
        "* la librairie NLTK (Natural Language Toolkit) : http://www.nltk.org\n",
        "* la librairie SpaCY : https://spacy.io/  \n",
        "\n",
        "Nous présentons comment ces dernières peuvent être utilisées pour réaliser les différents pré-traitements. \n",
        "L'importation de ces librairie se fait de la manière suivante :\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uamVDVPbsAnm"
      },
      "source": [
        "import nltk\n",
        "# il est possible de charger l'ensemble des librairies en une seule fois \n",
        "# décocher le commentaire de la ligne ci-dessous\n",
        "#nltk.download('all') \n",
        "\n",
        "import spacy\n",
        "# il faut sélectionner pour quelle langue les traitements vont être faits.\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPT-ZuAr7Tv3"
      },
      "source": [
        "import nltk\n",
        "# il est possible de charger l'ensemble des librairies en une seule fois \n",
        "# décocher le commentaire de la ligne ci-dessous\n",
        "#nltk.download('all') \n",
        "\n",
        "import spacy\n",
        "# il faut sélectionner pour quelle langue les traitements vont être faits.\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5xjESPsS15X"
      },
      "source": [
        "# Utilisation de NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq-GcuxMS9l-"
      },
      "source": [
        "NLTK (Natural Language Toolkit - http://www.nltk.org) est une bibliothèque Python développée par Steven Bird et Edward Loper du département d'informatique de l'université de Pennsylvanie. Elle offre de très nombreuses fonctionnalités pour manipuler les textes dans différentes langues dont le français.  \n",
        "\n",
        "L'importation de la librairie se fait par : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqwOvgoQUOxa"
      },
      "source": [
        "import nltk\n",
        "# il est possible de charger l'ensemble des librairies associées en une seule fois \n",
        "# pour cela décocher le commentaire de la ligne ci-dessous\n",
        "#nltk.download('all') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecKE29A-5sjh"
      },
      "source": [
        "document = \"Alice opened the door and found that it led into a small passage, not much larger than a rat-hole: she knelt down and looked along the passage into the loveliest garden you ever saw. \\\n",
        "How she longed to get out of that dark hall, and wander about among those beds of bright flowers and those cool fountains, but she could not even get her head through the doorway\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESXVKF5WtLDN"
      },
      "source": [
        "Sous NLTK,  le découpage en phrase peut se faire  à l'aide de la fonction *sent_tokenize* :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0CN9OB_5sjm"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "phrases = sent_tokenize(document)\n",
        "for phrase_nltk in phrases:\n",
        "  print (\"phrases : \",phrase_nltk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX_SJIgW-vnC"
      },
      "source": [
        "**Découpage en tokens (tokenisation)**  \n",
        "\n",
        "Un texte sous python est généralement considéré comme *string*. Il est donc tout à fait possible  d'utiliser les fonctions associées comme *lower* (conversion en minuscule) ou la fonction *split* associée pour découper en tokens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdzZv1kDthuc"
      },
      "source": [
        "print (\"conversion document en minuscule\")\n",
        "print (document.lower())\n",
        "\n",
        "document_splitted = document.split()\n",
        "print(document_splitted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5_8iCP3uG3l"
      },
      "source": [
        "\n",
        "Via NLTK, le découpage en tokens se fait via la fonction *word_tokenize*. Contrairement à la fonction *split* précédente, les caractères de ponctuations sont considérés comme tokens.   \n",
        "\n",
        "**Remarque :** comme nous pouvons le constater les ponctuations sont soient intégrées au dernier mot (*split*), soient correspondent à des tokens. Nous verrons jusque après que NTLK peut les reconnaître spécifiquement via une analyse grammaticale. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZKJ2xhB--qO"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# la liste des tokens de la première phrase\n",
        "tokens = word_tokenize(phrases[0])\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dwrxOgighfE"
      },
      "source": [
        "**Etiquetage grammatical** (*Part of Speech Tagging*)  \n",
        "\n",
        "L'étiquetage morpho-syntaxique (ou étiquetage grammatical) permet d'associer à chaque mot d'un texte les informations grammaticales correpondantes (e.g. verbe, préposition, ...).  \n",
        "Elle se fait via la fonction *pos_tag*. Elle s'applique à une phrase composée d'un ensemble de tokens et retourne les différents composants de la phrase. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2Fy2-dSgfmd"
      },
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "for phrase_nltk in phrases:\n",
        "  print (\"phrases : \",phrase_nltk)\n",
        "  tokens = word_tokenize(phrase_nltk)\n",
        "  tokens_tag = nltk.pos_tag(tokens)\n",
        "  print (tokens_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OLkR3QJt46-"
      },
      "source": [
        "Il est possible de connaître la liste de tous les tags disponibles :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AVi5f7yuBGo"
      },
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNqM9l5WwKxL"
      },
      "source": [
        "A partir de cet étiquetage, il est donc possible de ne sélectionner que des tokens correspondant à une catégorie. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCEm1hPTr7sX"
      },
      "source": [
        "word_tokens = word_tokenize(document)\n",
        "pos = nltk.pos_tag(word_tokens)\n",
        "selective_pos = ['NN','VBD']\n",
        "selective_pos_words = []\n",
        "for word,tag in pos:\n",
        "     if tag in selective_pos:\n",
        "         selective_pos_words.append((word,tag))\n",
        "print(selective_pos_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JglpVm2u0Qnh"
      },
      "source": [
        "**Mots vides** *Stop words*  \n",
        "\n",
        "Les mots vides correspondent à des mots qui sont tellement commun qu'il n'est pas nécessaire de les considérer dans l'apprentissage. NLTK possède une liste prédéfinie de mots vides faisant référence aux mots les plus courants. La première fois, il est nécessaire de télécharger les mots vides en utilisant : *nltk.download («stopwords»)*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl_LXV490nlr"
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "the_stopwords=set(stopwords.words(\"english\"))\n",
        "print (the_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5eUDRh9yf2d"
      },
      "source": [
        "Il est tout à fait possible de supprimer des stopwords de cette liste : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cPB2LflylSr"
      },
      "source": [
        "not_stopwords = {'this', 'd', 'o'} \n",
        "#new_stopwords_list=stopwords\n",
        "final_stop_words = set([word for word in the_stopwords if word not in not_stopwords])\n",
        "\n",
        "print (final_stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI8Al_lEx_Q2"
      },
      "source": [
        "ou d'étendre la liste des stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0RxqffXyD61"
      },
      "source": [
        "new_stopwords=['stopword1', 'stopword2']\n",
        "final_stop_words=final_stop_words.union(new_stopwords)\n",
        "\n",
        "print (final_stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhEqqs-009Qx"
      },
      "source": [
        "Pour supprimer les stopwords d'un document, il suffit de rechercher les tokens qui sont inclus dans les stopwords et de les supprimer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWY3SXt61Fg5"
      },
      "source": [
        "print (\"Avant suppression des stopwords\")\n",
        "print (word_tokens)\n",
        "tokens_Alice=[word for word in word_tokens if word not in the_stopwords]\n",
        "print (\"Après suppression des stopwords\")\n",
        "print (tokens_Alice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma3Z6aCvW79I"
      },
      "source": [
        "**Racinisation** (*stemming*) **et lemmatisation**\n",
        "\n",
        "NLTK utilise l'algorithme de racinisation de Porter et propose une fonction de lemmatisation. Il s'agit de deux approches différentes de transformation des flexions en leur radical ou racine. Voir https://fr.wikipedia.org/wiki/Racinisation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXLBzoABXAxZ"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "ps=nltk.stem.porter.PorterStemmer()\n",
        "\n",
        "print (\"tokens\")\n",
        "print (tokens_Alice)\n",
        "print(\"Stemming\")\n",
        "print([ps.stem(word) for word in tokens_Alice])\n",
        "\n",
        "print(\"Lemmatisation\")\n",
        "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "print([lem.lemmatize(word) for word in tokens_Alice])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl6QQQZpqth-"
      },
      "source": [
        "NLTK propose aussi un stemmatiseur pour le Français :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73HXoHysqnGy"
      },
      "source": [
        "# un autre stemmatiseur qui accepte le français\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"french\")\n",
        "phrase = \"malade malades maladie maladies maladive\"\n",
        "tokens = word_tokenize(phrase)\n",
        "print (\"Avant transformation \\n\")\n",
        "print (tokens)\n",
        "stemmed = [stemmer.stem(word) for word in tokens]\n",
        "print (\"\\n Après transformation\\n\")\n",
        "print (stemmed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7mMTp3LS5yS"
      },
      "source": [
        "# Utilisation de Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya8zmorUZbQQ"
      },
      "source": [
        "L'importation de la librairie se fait par : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00h8YV5TZeVs"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l5RetCtZUPs"
      },
      "source": [
        "Spacy utilise un objet particulier, généralement appelé *nlp*, qui va créer un pipeline sur tous les éléments d'un document afin de générer un objet de type doc. Le pipeline de base est le suivant :  \n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/PipelineSpacy.png\" align=\"center\" >\n",
        "Il existe bien entendu différents modèles de pipeline en fonction de la langue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lB8dx1PV-g8"
      },
      "source": [
        "Les ressources sont par défaut en anglais. Pour le français, il faut au préalable télécharger la ressource associée par : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbYu4buHWD9l"
      },
      "source": [
        "!python -m spacy download fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz_himxdZYiF"
      },
      "source": [
        "from spacy.lang.fr import French\n",
        "\n",
        "# Création d'un objet nlp\n",
        "nlp = French()\n",
        "\n",
        "# Créé en traitant une chaine de caractères avec l'objet nlp\n",
        "doc = nlp(\"Bonjour monde !\")\n",
        "\n",
        "# Itère sur les tokens dans un Doc\n",
        "for token in doc:\n",
        "    print(token.text)\n",
        "\n",
        "# remise des ressources en anglais pour la suite\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly6B3-khMhXp"
      },
      "source": [
        "document = \"Alice opened the door and found that it led into a small passage, not much larger than a rat-hole: she knelt down and looked along the passage into the loveliest garden you ever saw. \\\n",
        "How she longed to get out of that dark hall, and wander about among those beds of bright flowers and those cool fountains, but she could not even get her head through the doorway\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIf8lwzg-g-G"
      },
      "source": [
        "**Découpage en phrases, tokenisation et analyse grammaticale**  \n",
        "\n",
        "Le découpage en phrases se fait via l'attribut (*sents*) lors de la création du pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg3RUCK6803b"
      },
      "source": [
        "# le document est en anglais\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "doc=nlp(document)\n",
        "for phrases_spacy in doc.sents:\n",
        "  print (\"phrases : \", phrases_spacy)\n",
        "\n",
        "#sauvegarde des phrases dans un tableau pour les manipulations ultérieures\n",
        "sentences = [sent.string.strip() for sent in doc.sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB_yPVAmM0J3"
      },
      "source": [
        "Spacy permet d'avoir de très nombreuses information sur les tokens : le token, l'index (associé au nombre de caractères), le lemme associé (voir plus loin), s'il s'agit d'un caractère de ponctuation, d'un espace, la forme (un X représente une majuscule et un x une minuscule), sa catégorie (Part of Speech tagging), le tag associé, s'il s'agit dune entité nommée, etc.\n",
        "\n",
        "La liste des différents attributs est disponible ici : https://spacy.io/api/token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rew84aYgEwXH"
      },
      "source": [
        "#traitement de la première phrase\n",
        "first_sentence=nlp(sentences[0])\n",
        "\n",
        "for token in first_sentence:\n",
        "    # l'objet token contient différents attributs\n",
        "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\t{8}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.shape_,\n",
        "        token.pos_,\n",
        "        token.tag_,\n",
        "        token.ent_type_\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWAOK2Sybnhc"
      },
      "source": [
        "Il est également possible de visualiser les résultats de l'analyse grammaticale : \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psqfIi5gbxkH"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(first_sentence, style='dep', jupyter=True, options={'distance': 85})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jffNzzpQZ2yR"
      },
      "source": [
        "**Entité nommée** (*name entity*)   \n",
        "\n",
        "Spacy permet également d'extraire les entités nommées d'un texte. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsnMKGgpckwc"
      },
      "source": [
        "example_withnamedentities=\"Donald Trump was a President of the US and now it is Joe Biden\"\n",
        "sentence=nlp(example_withnamedentities)\n",
        "for entity in sentence.ents:\n",
        "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtdUUWOVhQDY"
      },
      "source": [
        "## **D'autres librairies ou traitements pratiques**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7eY5_qfhV4f"
      },
      "source": [
        "Nous présentons ici différentes librairies ou traitements souvents utilisés. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5SDQeI9hgzf"
      },
      "source": [
        "#inflect est une librairie qui permet de convertir les nombres en mots\n",
        "import inflect\n",
        "\n",
        "phrase=\"They are 100\"\n",
        "tokens = word_tokenize(phrase)\n",
        "\n",
        "print (\"Nombre à convertir \\n\")\n",
        "words = [word for word in tokens if word.isdigit()]\n",
        "print(words)\n",
        "p = inflect.engine()\n",
        "numbertransf = [p.number_to_words(word) for word in tokens if word.isdigit()]\n",
        "\n",
        "print (\"Nombre après conversion \\n\")\n",
        "print(numbertransf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ah10V8yiIGS"
      },
      "source": [
        "tokens = [w.lower() for w in tokens]\n",
        "print (tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l97Nw4imipwo"
      },
      "source": [
        "# Suppression de tous les termes qui ne sont pas alphanumériques\n",
        "words = [word for word in tokens if word.isalpha()]\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWqR9i-eis3H"
      },
      "source": [
        "import contractions\n",
        "\n",
        "phrase=\"They're 100\"\n",
        "tokens = word_tokenize(phrase)\n",
        "\n",
        "def replace_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "print (\"Avant remplacement\\n\")\n",
        "print (phrase)\n",
        "print (\"\\nAprès remplacement\\n\")\n",
        "laphrase=replace_contractions(phrase)\n",
        "print (laphrase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Pba4ZsjtvR"
      },
      "source": [
        "Les tweets ont une syntaxe très particulière et généralement les traitements se font à l'aide d'expressions régulières."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxAx0BIZjv1i"
      },
      "source": [
        "import re\n",
        "tweet = '#ML is thus a good example :D ;) RT @theUser: see http://ml.example.com'\n",
        "#traitement des émoticones\n",
        "emoticons_str = r\"\"\"\n",
        "    (?:\n",
        "        [:=;] # Eyes\n",
        "        [oO\\-]? # Nose (optional)\n",
        "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
        "    )\"\"\"\n",
        "\n",
        "#Prise en compte des éléments qui doivent être regroupés\n",
        "regex_str = [\n",
        "    emoticons_str,\n",
        "    r'<[^>]+>', # HTML tags\n",
        "    r'(?:@[\\w_]+)', # @-mentions\n",
        "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
        "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
        " \n",
        "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # nombres\n",
        "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # mots avec - et '\n",
        "    r'(?:[\\w_]+)', # autres mots\n",
        "    r'(?:\\S)' # le reste\n",
        "]\n",
        "    \n",
        "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
        " \n",
        "def tokenize(s):\n",
        "    return tokens_re.findall(s)\n",
        " \n",
        "def preprocess(s, lowercase=False):\n",
        "    tokens = tokenize(s)\n",
        "    if lowercase:\n",
        "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
        "    return tokens\n",
        " \n",
        "# un example de tweet\n",
        "\n",
        "print (\"Un exemple de tweet : \\n\",tweet)\n",
        "\n",
        "print (\"\\nLe tweet avec un processus normal de transformation\\n\")\n",
        "print (word_tokenize(tweet))       \n",
        "print (\"\\nLe tweet avec des expressions régulières\\n\")       \n",
        "words=preprocess(tweet)       \n",
        "print(words)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuVY2E338Abu"
      },
      "source": [
        "# **Une petite mise en pratique**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyh2tu1Z8Orq"
      },
      "source": [
        "Il est temps à présent de mettre en pratique ce que nous avons vu. \n",
        "\n",
        "Considérez le document suivant : \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg5a1eVNfy7D"
      },
      "source": [
        "testpratique=[u\"\"\"“Curiouser and curiouser!” cried Alice (she was so much surprised, that for the moment she quite forgot how to speak good English); “now I’m opening out like the largest telescope that ever was! Good-bye, feet!” (for when she looked down at her feet, they seemed to be almost out of sight, they were getting so far off). “Oh, my poor little feet, I wonder who will put on your shoes and stockings for you now, dears? I’m sure I shan’t be able! I shall be a great deal too far off to trouble myself about you: you must manage the best way you can;—but I must be kind to them,” thought Alice, “or perhaps they won’t walk the way I want to go! Let me see: I’ll give them a new pair of boots every Christmas.”\n",
        "And she went on planning to herself how she would manage it. “They must go by the carrier,” she thought; “and how funny it’ll seem, sending presents to one’s own feet! And how odd the directions will look!\n",
        "     Alice’s Right Foot, Esq.,\n",
        "       Hearthrug,\n",
        "         near the Fender,\n",
        "           (with Alice’s love).\n",
        "Oh dear, what nonsense I’m talking!”\n",
        "\"\"\",u\"\"\"After a time she heard a little pattering of feet in the distance, and she hastily dried her eyes to see what was coming. It was the White Rabbit returning, splendidly dressed, with a pair of white kid gloves in one hand and a large fan in the other: he came trotting along in a great hurry, muttering to himself as he came, “Oh! the Duchess, the Duchess! Oh! won’t she be savage if I’ve kept her waiting!” Alice felt so desperate that she was ready to ask help of any one; so, when the Rabbit came near her, she began, in a low, timid voice, “If you please, sir—” The Rabbit started violently, dropped the white kid gloves and the fan, and skurried away into the darkness as hard as he could go.\"\"\",\n",
        "u\"\"\"They were indeed a queer-looking party that assembled on the bank—the birds with draggled feathers, the animals with their fur clinging close to them, and all dripping wet, cross, and uncomfortable.\n",
        "The first question of course was, how to get dry again: they had a consultation about this, and after a few minutes it seemed quite natural to Alice to find herself talking familiarly with them, as if she had known them all her life. Indeed, she had quite a long argument with the Lory, who at last turned sulky, and would only say, “I am older than you, and must know better;” and this Alice would not allow without knowing how old it was, and, as the Lory positively refused to tell its age, there was no more to be said.\n",
        "At last the Mouse, who seemed to be a person of authority among them, called out, “Sit down, all of you, and listen to me! I’ll soon make you dry enough!” They all sat down at once, in a large ring, with the Mouse in the middle. Alice kept her eyes anxiously fixed on it, for she felt sure she would catch a bad cold if she did not get dry very soon.\n",
        "“Ahem!” said the Mouse with an important air, “are you all ready? This is the driest thing I know. Silence all round, if you please! ‘William the Conqueror, whose cause was favoured by the pope, was soon submitted to by the English, who wanted leaders, and had been of late much accustomed to usurpation and conquest. Edwin and Morcar, the earls of Mercia and Northumbria—’”\n",
        "“Ugh!” said the Lory, with a shiver.\n",
        "“I beg your pardon!” said the Mouse, frowning, but very politely: “Did you speak?”\n",
        "“Not I!” said the Lory hastily.\n",
        "“I thought you did,” said the Mouse. “—I proceed. ‘Edwin and Morcar, the earls of Mercia and Northumbria, declared for him: and even Stigand, the patriotic archbishop of Canterbury, found it advisable—’”\n",
        "“Found what?” said the Duck.\n",
        "“Found it,” the Mouse replied rather crossly: “of course you know what ‘it’ means.”\n",
        "“I know what ‘it’ means well enough, when I find a thing,” said the Duck: “it’s generally a frog or a worm. The question is, what did the archbishop find?”\"\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNQDWORj5sjL"
      },
      "source": [
        "Il contient différentes phrases plus ou moins longues. Vous pourrez réaliser les opérations en utilisant soit NLTK ou Spacy.\n",
        "\n",
        "<font color=red>Exercice :</font> \n",
        "1. Afficher les wordclouds associés aux documents\n",
        "1. Transformer les documents de telle sorte qu'ils soient en minuscule, qu'il ne possède plus de caractères spéciaux ni uniques.\n",
        "1. Transformer les en tokens de manière à ce qu'ils ne contiennent plus que des tokens de type NN et VB.\n",
        "1. Enfin, transformer les tokens pour n'avoir que leur racine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syMgVJ4ip02N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V42wtvpDp1ic"
      },
      "source": [
        "<font color=blue>Solution :</font>  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nps6hgpDEJFd"
      },
      "source": [
        "wc = wordcloud.WordCloud(background_color='black', max_words=100, \n",
        "                         max_font_size=35)\n",
        "wc = wc.generate(str(testpratique))\n",
        "fig = plt.figure(num=1)\n",
        "plt.axis('off')\n",
        "plt.imshow(wc, cmap=None)\n",
        "plt.show()\n",
        "\n",
        "print (\"Document initial \", testpratique)\n",
        "# suppression des caractères spéciaux\n",
        "sentence = re.sub(r'[^\\w\\s]',' ', str(testpratique))\n",
        "# suppression de tous les caractères uniques\n",
        "sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
        "\n",
        "# tokenisation\n",
        "word_tokens = word_tokenize(sentence)\n",
        "print (\"Premiers tokens après nettoyage des données \", word_tokens)\n",
        "\n",
        "# ne retenir que les categories NN et VB\n",
        "# sauvegarde des mots dans un tableau selective_words. \n",
        "# selective_pos_words contient le mot et son tag\n",
        "pos = nltk.pos_tag(word_tokens)\n",
        "selective_pos = ['NN','VB']\n",
        "selective_pos_words = []\n",
        "selective_words=[]\n",
        "for word,tag in pos:\n",
        "     if tag in selective_pos:\n",
        "         selective_pos_words.append((word,tag))\n",
        "         selective_words.append(word)\n",
        "\n",
        "print (\"tokens conservés de catégorie NN ou VB\")\n",
        "print (selective_pos_words)\n",
        "# racinisation\n",
        "\n",
        "\n",
        "ps=nltk.stem.porter.PorterStemmer()\n",
        "\n",
        "\n",
        "print(\"Racinisation\")\n",
        "print([ps.stem(word) for word in selective_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZE6wRZ9rBWJ"
      },
      "source": [
        "Nous savons maintenant nettoyer nos données et les transformer sous la forme de tokens. Aussi nous allons comment ces derniers peuvent être utilisés comme représentation des documents pour faire .... de la classification. La classification de données textuelles est présentée dans un autre notebook. "
      ]
    }
  ]
}